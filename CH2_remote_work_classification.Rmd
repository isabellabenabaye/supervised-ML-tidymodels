---
title: 'Chapter 2: Stack Overflow  Developer Survey'
author: "Isabella Benabaye"
date: "7/3/2020"
output: html_document
---
```{r imports}
library(tidyverse)
library(tidymodels)
library(hrbrthemes)
library(extrafont)
loadfonts(device = "win", quiet = TRUE) ## to load the font
theme_set(theme_ipsum(base_family = "IBM Plex Sans", base_size = 13, axis_title_size = 14))
  
```

# Import data
```{r}
stack_overflow <- read_csv("https://raw.githubusercontent.com/juliasilge/supervised-ML-case-studies-course/master/data/stack_overflow.csv")
```

# Explore the Stack Overflow survey
```{r}
glimpse(stack_overflow)

# Count for 'remote'
stack_overflow %>% 
  count(remote, sort = TRUE)

# Count for 'country'
stack_overflow %>% 
  count(country, sort = TRUE)
```

Remote: there are way more people who don't work remotely than those that do. The dataset is imbalanced.

Country: Most of the respondents are from the U.S. (~50%), then the U.K. (18%). The rest of the 32% of the respondents are from Germany, India, and Canada.

```{r}
stack_overflow %>% 
  ggplot(aes(remote, years_coded_job)) +
  geom_boxplot() +
  labs(x = NULL, y = "Years of professional coding experience")
```

Those that work remotely have more varied years of experience and a higher median, while those that don't work remotely are more centered at those who have less years of experience. Both groups have the same range though, from those who have 0 to 20 years of experience.

# Training and testing data
Splitting the data into training and testing sets to reduce overfitting and obtain a more accurate estimate for how the model will perform on new data.

Split the data into 80%/20% sections and evenly divide the training and testing data between the two classes of `remote`.

```{r}
# Remove the identifier column
stack_select <- stack_overflow %>% 
  select(-respondent)

# Split the data
set.seed(1234)
stack_split <- stack_select %>% 
  initial_split(prop = .8, strata = remote)

stack_train <- training(stack_split)
stack_test <- testing(stack_split)

glimpse(stack_train)
glimpse(stack_test)
```

# Preprocess the training data with a recipe
For this case study, the preprocessing method we will use is **downsampling**.

First, we'll create and define the recipe:
```{r}
stack_recipe <- recipe(remote ~ ., data = stack_train) %>% 
  step_downsample(remote)
  
stack_recipe
```

Now we'll estimate the parameters required to preprocess the data (`prep()`) then extract the preprocessed data (`juice()`). This step isn't necessary since you use a `workflow()`for modeling, but it's helpful to diagnose problems and explore the preprocessing results.

```{r}
# Estimate the parameters required to preprocess the data
stack_prep <- prep(stack_recipe)
# Extract the processed data
stack_down <- juice(stack_prep)

stack_down %>% 
  count(remote)
```

