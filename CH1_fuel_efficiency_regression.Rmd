---
title: 'Chapter 1: Not mtcars AGAIN'
author: "Isabella Benabaye"
date: "6/23/2020"
output: html_document
---
```{r imports}
library(tidyverse)
library(tidymodels)
library(extrafont)
loadfonts(device = "win", quiet = TRUE) ## to load the font
```

# Chapter 1
## Visualize the fuel efficiency distribution
Exploring the data
```{r visualize fuel efficiency}
cars2018 <- read_csv("https://raw.githubusercontent.com/juliasilge/supervised-ML-case-studies-course/master/data/cars2018.csv") %>% 
  janitor::clean_names()  ## clean the variable names

glimpse(cars2018)

ggplot(cars2018, aes(mpg)) +
  geom_histogram(bins = 25) +
  labs(x = "Fuel efficiency (mpg)",
       y = "Number of cars") +
  hrbrthemes::theme_ipsum(base_family = "IBM Plex Sans")
```

## Build a simple linear model
Build the simplest model to get an idea of what's going on. In this case we can fit a simple linear model using base R's `lm()` function.

Remove the two columns `Model` and `Model Index` since they tell us individual identifiers for each car that won't be used for modeling.
```{r simple model}
car_vars <- cars2018 %>% 
  select(-model, -model_index)
```

Fit `MPG` as the predicted quantity, explained by all other predictors.
```{r lm - fit all}
fit_all <- lm(mpg ~ ., data = car_vars)
```

Print the summary of the model.
```{r lm - summary}
summary(fit_all)
```

## Getting training and testing data with `rsample`
Creating training/testing splits reduces overfitting. When you evaluate your model on data that it was not trained on, you get a better estimate of how it will perform on new data.

**Balanced sets:** 
80% for training, with cases chosen so that both sets are balanced in `Transmission` types
```{r split data}
set.seed(1234)
car_split <- car_vars %>% 
  initial_split(prop = 0.8,
                strata = transmission)

car_train <- training(car_split)
car_test <- testing(car_split)

glimpse(car_train)
glimpse(car_test)
```

## Training a model (no resampling)
When we model data, we deal with model type (such as linear regression or random forest), mode (regression or classification), and model engine (how the models are actually fit). In tidymodels, we capture that modeling information in a model specification, so setting up your model specification can be a good place to start. 

Three concepts in specifying a model: 
Model type - differentiates models such as logistic regression, decision tree models, and so forth
Model mode - includes common options like regression and classification; some model types support either of these while some only have one mode
Model engine - the computational tool which will be used to fit the model

After a model has been specified, it can be fit, typically using a symbolic description of the model (a formula) and some data. We're going to start fitting models with data = car_train, as shown here. This means we're saying, "Just fit the model one time, on the whole training set". Once you have fit your model, you can evaluate how well the model is performing.

We'll fit the `log(MPG)` since the fuel efficiency has a log normal distribution.

**Linear regression model specification**
```{r tidy - lm spec}
lm_mod  <- linear_reg() %>% 
  set_engine("lm")

# Fit the model with a log transformed y
fit_lm <- lm_mod %>% 
  fit(log(mpg) ~ .,
      data = car_train)

fit_lm
```

**Random forest model specification**
```{r tidy - random forest spec}
rf_mod <- rand_forest() %>% 
  set_mode("regression") %>% 
  set_engine("randomForest")

fit_rf <- rf_mod %>% 
  fit(log(mpg) ~ .,
      data = car_train)

fit_rf
```

## Evaluating model performance
We will use the package {yardstick} to give us metrics to measure how well our models are doing.

For regression models, we will focus on evaluating using the root mean squared error metric. This quantity is measured in the same units as the original data (log of miles per gallon, in our case). Lower values indicate a better fit to the data. Itâ€™s not too hard to calculate root mean squared error manually, but the yardstick package offers convenient functions for this and many other model performance metrics.

Create new columns for model predictions of the test data from each of the models we have trained.
```{r evaluating model performance 1}
results <- car_test %>% 
  mutate(mpg = log(mpg)) %>% 
  bind_cols(predict(fit_lm, car_test) %>% 
              rename(.pred_lm = .pred)) %>% 
  bind_cols(predict(fit_rf, car_test) %>% 
              rename(.pred_rf = .pred))
```

Evaluate the performance of the predictions using `metrics()` by specifying the column that contains the real fuel efficiency.
```{r evaluating model performance 2}
metrics(results, truth = mpg, estimate = .pred_lm)
metrics(results, truth = mpg, estimate = .pred_rf)
```

## Bootstrap resampling with tidymodels
Resampling can help us evaluate our machine learning models more accurately.

There are functions such as `bootstraps()` and similar for other types of resampling. The default behavior is to do 25 bootstrap resamplings, but you can change this if you want to. The column splits is of type list. Instead of containing numbers or characters, this column contains lists. Each split in that column keeps track of which of the original data points are in the analysis set for that 

**Create bootstrap samples**
```{r bootstrap resamples}
car_boot <- bootstraps(car_train)
```

**Evaluate the models with bootstrap resampling**
```{r}
lm_res <- lm_mod %>% 
  fit_resamples(
    log(mpg) ~ .,
    resamples = car_boot,
    control = control_resamples(save_pred = TRUE)
  )

rf_res <- rf_mod %>% 
  fit_resamples(
    log(mpg) ~ .,
    resamples = car_boot,
    control = control_resamples(save_pred = TRUE)
  )

glimpse(rf_res)
```


Once you have created a set of resamples, you can use the function fit_resamples() to fit a model to each resample and compute performance metrics for each.

The code on this slide shows how to fit our model specification lm_mod to the 25 bootstrap resamples in car_boot. This will fit our regression model 25 times, each time to a different bootstrapped version of the training data. We also determine how well our regression model performed 25 times, each time on the smaller subset of training data set aside when fitting. The fitted models themselves are just thrown away and not stored in the output, because they are only used for computing performance metrics.

We will not save the fitted models but we are going to save our predictions in `fit_resamples()` using save_pred = TRUE. This is so we can be especially clear about what it is that we are comparing during this process.

Each car has a real fuel efficiency as reported by the Department of Energy and then we have built models that predict fuel efficiency for each car. When we evaluate a model, we are calculating how far apart each predicted value is from each real value.

## Look at the predictions
Notice in this code how we use bind_rows() from dplyr to combine the results from both models, along with collect_predictions() to obtain and format predictions from each resample.

```{r}
results <- bind_rows(lm_res %>% 
                       collect_predictions() %>% 
                       mutate(model = "lm"),
                     rf_res %>% 
                       collect_predictions() %>% 
                       mutate(model = "rf"))

glimpse(results)
```


## Visualizing model predictions

The x-axis has the actual fuel efficiency and the y-axis has the predicted fuel efficiency for each kind of model.

The difference between linear regression and random forest isn't huge here, but in this case, we can see visually that the random forest model is performing better. The slope for the random forest model is closer to the dotted line (the slope = 1 line) and the spread around the line is smaller for the random forest model.

```{r}
results %>% 
  ggplot(aes(`log(mpg)`, .pred)) +
  geom_abline(lty = 2, color = "gray50") +
  geom_point(aes(color = id), size = 1.5, alpha = 0.3, show.legend = FALSE) +
  geom_smooth(method = "lm") +
  facet_wrap(~ model) +
  hrbrthemes::theme_ipsum(base_family = "IBM Plex Sans")
  
```

